{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b743727-7759-44b4-89bf-aaafb28b8350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ecommerce_catalog;\n",
    "USE SCHEMA ecommerce_bronze;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a4d863-299d-48ef-adb8-ae907343bc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3f9d53-d85e-470e-a710-75e44ee254c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Silver_customers = spark.read.table(\n",
    "    \"ecommerce_catalog.ecommerce_bronze.customers\"\n",
    ")\n",
    "display(Silver_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2897a6a0-600d-4c12-b1e1-155728f6b537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Silver_customers.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "82f4ad0d-152f-43d0-ab48-cfbbb9dc7c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_customers_dedup = Silver_customers.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "display(silver_customers_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8686724a-5b6f-412c-8c90-f9978f41348d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_customers_dedup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15310d80-ae81-4548-a791-117f258f78d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, row_number, current_timestamp, lit, trim,\n",
    "    when, lower, regexp_replace, length, concat\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"customers\"\n",
    "BATCH_ID = \"silver_customers_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "DEFAULT_COUNTRY_CODE = \"91\"\n",
    "MIN_PHONE_LENGTH = 10\n",
    "MAX_PHONE_LENGTH = 15\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by customer_id using ingestion timestamp\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Standardize gender\n",
    "df = df.withColumn(\n",
    "    \"gender\",\n",
    "    when(lower(col(\"gender\")).isin(\"m\", \"male\"), \"Male\")\n",
    "    .when(lower(col(\"gender\")).isin(\"f\", \"female\"), \"Female\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Handle city and country\n",
    "df = df.withColumn(\"city\", when(col(\"city\").isNull(), \"Unknown\").otherwise(col(\"city\"))) \\\n",
    "       .withColumn(\"country\", when(col(\"country\").isNull(), \"Unknown\").otherwise(col(\"country\")))\n",
    "\n",
    "# Normalize telephone\n",
    "df = df.withColumn(\"telephone_clean\", regexp_replace(col(\"telephone\"), \"[^0-9]\", \"\")) \\\n",
    "       .withColumn(\n",
    "           \"telephone\",\n",
    "           when(\n",
    "               (length(col(\"telephone_clean\")) >= MIN_PHONE_LENGTH) &\n",
    "               (length(col(\"telephone_clean\")) <= MAX_PHONE_LENGTH),\n",
    "               when(\n",
    "                   col(\"telephone_clean\").startswith(DEFAULT_COUNTRY_CODE),\n",
    "                   concat(lit(\"+\"), col(\"telephone_clean\"))\n",
    "               ).otherwise(\n",
    "                   concat(lit(\"+\"), lit(DEFAULT_COUNTRY_CODE), col(\"telephone_clean\"))\n",
    "               )\n",
    "           )\n",
    "       ).drop(\"telephone_clean\")\n",
    "\n",
    "# Default job title\n",
    "df = df.withColumn(\"job_title\", when(col(\"job_title\").isNull(), \"Unknown\").otherwise(col(\"job_title\")))\n",
    "\n",
    "# Add audit columns\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d0c8aa-2b1b-48af-a75f-b97990ab3cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DISCOUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8bb77f-dd3c-4776-80e5-5073babf6498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Silver_discounts = spark.read.table(\n",
    "    \"ecommerce_catalog.ecommerce_bronze.discounts\"\n",
    ")\n",
    "display(Silver_discounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf28c8b9-54be-4043-8cd7-fd9fe2629dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit, regexp_extract\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"discounts\"\n",
    "BATCH_ID = \"silver_discounts_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Rename discont â†’ discount_value if exists\n",
    "if \"discont\" in df.columns:\n",
    "    df = df.withColumnRenamed(\"discont\", \"discount_value\")\n",
    "\n",
    "# Ensure start and end are timestamp\n",
    "df = df.withColumn(\"start\", col(\"start\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"end\", col(\"end\").cast(\"timestamp\"))\n",
    "\n",
    "# Parse discount_value from description if missing\n",
    "df = df.withColumn(\n",
    "    \"discount_value\",\n",
    "    when(\n",
    "        col(\"discount_value\").isNull(),\n",
    "        regexp_extract(col(\"description\"), r\"(\\d+)%\", 1).cast(\"double\") / 100\n",
    "    ).otherwise(col(\"discount_value\"))\n",
    ")\n",
    "\n",
    "# Handle null description\n",
    "df = df.withColumn(\n",
    "    \"description\",\n",
    "    when(col(\"description\").isNull(), \"No description provided\").otherwise(col(\"description\"))\n",
    ")\n",
    "\n",
    "# Deduplicate if discount_id exists\n",
    "if \"discount_id\" in df.columns:\n",
    "    df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "    window_spec = Window.partitionBy(\"discount_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "    df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "           .filter(col(\"rn\") == 1) \\\n",
    "           .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Add audit columns\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a28f3d-d53a-42db-ba22-774492b59c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EMPLOYEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "774d56bd-c4fc-4886-b542-fbb8f0efdbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"employees\"\n",
    "BATCH_ID = \"silver_employees_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"employee_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by employee_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"employee_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls for string columns\n",
    "df = df.withColumn(\"name\", when(col(\"name\").isNull(), \"Unknown\").otherwise(col(\"name\"))) \\\n",
    "       .withColumn(\"position\", when(col(\"position\").isNull(), \"Unknown\").otherwise(col(\"position\"))) \\\n",
    "       .withColumn(\"source_file\", when(col(\"source_file\").isNull(), \"Unknown\").otherwise(col(\"source_file\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2eb4fc9-fd8e-41c0-96fa-8e8fbeebba6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff0f1e9-2c02-4f4f-a1f1-17b05046c694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"products\"\n",
    "BATCH_ID = \"silver_products_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by product_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\n",
    "    \"category\", \"sub_category\", \"description_pt\", \"description_de\",\n",
    "    \"description_fr\", \"description_es\", \"description_en\", \"description_zh\",\n",
    "    \"color\", \"sizes\", \"source_file\"\n",
    "]\n",
    "\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle null production_cost\n",
    "df = df.withColumn(\"production_cost\", when(col(\"production_cost\").isNull(), 0.0).otherwise(col(\"production_cost\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa59037f-de98-419b-89ee-9ed35f9540c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f9a934-96e3-435d-a83c-82d94bba923f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"stores\"\n",
    "BATCH_ID = \"silver_stores_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"store_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by store_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"store_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\"country\", \"city\", \"store_name\", \"zip_code\", \"source_file\"]\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle nulls / default values for numeric columns\n",
    "df = df.withColumn(\"number_of_employees\", when(col(\"number_of_employees\").isNull(), 0).otherwise(col(\"number_of_employees\")))\n",
    "df = df.withColumn(\"latitude\", when(col(\"latitude\").isNull(), 0.0).otherwise(col(\"latitude\")))\n",
    "df = df.withColumn(\"longitude\", when(col(\"longitude\").isNull(), 0.0).otherwise(col(\"longitude\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a9ba93-63ee-4aa0-beda-2d3bba123718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TRANSACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a56d68e-8b12-4689-8110-77e7aed73562",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768373439246}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"transactions\"\n",
    "BATCH_ID = \"silver_transactions_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary keys (invoice_id & line)\n",
    "df = df.filter(col(\"invoice_id\").isNotNull() & col(\"line\").isNotNull())\n",
    "\n",
    "# Deduplicate using composite key: invoice_id + line\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"invoice_id\", \"line\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\n",
    "    \"size\", \"color\", \"currency\", \"currency_symbol\", \"sku\",\n",
    "    \"transaction_type\", \"payment_method\"\n",
    "]\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle nulls / default values for numeric columns\n",
    "numeric_columns = [\n",
    "    \"unit_price\", \"quantity\", \"discount\", \"line_total\", \n",
    "    \"customer_id\", \"product_id\", \"store_id\", \"employee_id\", \"invoice_total\"\n",
    "]\n",
    "for c in numeric_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), 0).otherwise(col(c)))\n",
    "\n",
    "# Handle null date\n",
    "df = df.withColumn(\"date\", when(col(\"date\").isNull(), current_timestamp()).otherwise(col(\"date\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4547324177522605,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
