{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7b743727-7759-44b4-89bf-aaafb28b8350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ecommerce_catalog;\n",
    "USE SCHEMA ecommerce_bronze;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a4d863-299d-48ef-adb8-ae907343bc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91d5b16-f553-46db-8ba4-a07e3560035a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"customers\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read the table\n",
    "customers_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get row count\n",
    "customers_count = customers_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {customers_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15310d80-ae81-4548-a791-117f258f78d2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768884143274}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, row_number, current_timestamp, lit, trim,\n",
    "    when, lower, regexp_replace, length, concat\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"customers\"\n",
    "BATCH_ID = \"silver_customers_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "DEFAULT_COUNTRY_CODE = \"91\"\n",
    "MIN_PHONE_LENGTH = 10\n",
    "MAX_PHONE_LENGTH = 15\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "# -----------------------------\n",
    "df = df_bronze.select(\n",
    "    *[\n",
    "        when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c)\n",
    "        for c in df_bronze.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Remove invalid primary keys\n",
    "# -----------------------------\n",
    "df = df.filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ Correct Deduplication Logic\n",
    "# -----------------------------\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"ingestion_ts\").desc())\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rn\", row_number().over(window_spec))\n",
    "      .filter(col(\"rn\") == 1)\n",
    "      .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Standardize gender\n",
    "# -----------------------------\n",
    "df = df.withColumn(\n",
    "    \"gender\",\n",
    "    when(lower(col(\"gender\")).isin(\"m\", \"male\"), \"Male\")\n",
    "    .when(lower(col(\"gender\")).isin(\"f\", \"female\"), \"Female\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Handle city and country\n",
    "# -----------------------------\n",
    "df = (\n",
    "    df.withColumn(\"city\", when(col(\"city\").isNull(), \"Unknown\").otherwise(col(\"city\")))\n",
    "      .withColumn(\"country\", when(col(\"country\").isNull(), \"Unknown\").otherwise(col(\"country\")))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize telephone\n",
    "# -----------------------------\n",
    "df = (\n",
    "    df.withColumn(\"telephone_clean\", regexp_replace(col(\"telephone\"), \"[^0-9]\", \"\"))\n",
    "      .withColumn(\n",
    "          \"telephone\",\n",
    "          when(\n",
    "              (length(col(\"telephone_clean\")) >= MIN_PHONE_LENGTH) &\n",
    "              (length(col(\"telephone_clean\")) <= MAX_PHONE_LENGTH),\n",
    "              when(\n",
    "                  col(\"telephone_clean\").startswith(DEFAULT_COUNTRY_CODE),\n",
    "                  concat(lit(\"+\"), col(\"telephone_clean\"))\n",
    "              ).otherwise(\n",
    "                  concat(lit(\"+\"), lit(DEFAULT_COUNTRY_CODE), col(\"telephone_clean\"))\n",
    "              )\n",
    "          )\n",
    "      )\n",
    "      .drop(\"telephone_clean\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Default job title\n",
    "# -----------------------------\n",
    "df = df.withColumn(\n",
    "    \"job_title\",\n",
    "    when(col(\"job_title\").isNull(), \"Unknown\").otherwise(col(\"job_title\"))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = (\n",
    "    df.withColumn(\"batch_id\", lit(BATCH_ID))\n",
    "      .withColumn(\"record_created_at\", current_timestamp())\n",
    "      .withColumn(\"record_updated_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify no duplicates\n",
    "# -----------------------------\n",
    "display(\n",
    "    spark.table(SILVER_TABLE)\n",
    "         .groupBy(\"customer_id\")\n",
    "         .count()\n",
    "         .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfdc61e-b269-4816-ba29-1471c51019a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d0c8aa-2b1b-48af-a75f-b97990ab3cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DISCOUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d857a9a-5392-4e7d-bd18-67311650044b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"discounts\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read table\n",
    "discounts_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get count\n",
    "discounts_count = discounts_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {discounts_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf28c8b9-54be-4043-8cd7-fd9fe2629dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit, regexp_extract\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"discounts\"\n",
    "BATCH_ID = \"silver_discounts_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Rename discont → discount_value if exists\n",
    "if \"discont\" in df.columns:\n",
    "    df = df.withColumnRenamed(\"discont\", \"discount_value\")\n",
    "\n",
    "# Ensure start and end are timestamp\n",
    "df = df.withColumn(\"start\", col(\"start\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"end\", col(\"end\").cast(\"timestamp\"))\n",
    "\n",
    "# Parse discount_value from description if missing\n",
    "df = df.withColumn(\n",
    "    \"discount_value\",\n",
    "    when(\n",
    "        col(\"discount_value\").isNull(),\n",
    "        regexp_extract(col(\"description\"), r\"(\\d+)%\", 1).cast(\"double\") / 100\n",
    "    ).otherwise(col(\"discount_value\"))\n",
    ")\n",
    "\n",
    "# Handle null description\n",
    "df = df.withColumn(\n",
    "    \"description\",\n",
    "    when(col(\"description\").isNull(), \"No description provided\").otherwise(col(\"description\"))\n",
    ")\n",
    "\n",
    "# Deduplicate if discount_id exists\n",
    "if \"discount_id\" in df.columns:\n",
    "    df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "    window_spec = Window.partitionBy(\"discount_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "    df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "           .filter(col(\"rn\") == 1) \\\n",
    "           .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Add audit columns\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83856f0b-e0d9-48f6-8ee3-a84fe2d37867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a28f3d-d53a-42db-ba22-774492b59c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EMPLOYEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a28793a-9a20-495f-8f5d-df23f765dbcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"employees\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read table\n",
    "employees_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get row count\n",
    "employees_count = employees_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {employees_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774d56bd-c4fc-4886-b542-fbb8f0efdbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"employees\"\n",
    "BATCH_ID = \"silver_employees_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"employee_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by employee_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"employee_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls for string columns\n",
    "df = df.withColumn(\"name\", when(col(\"name\").isNull(), \"Unknown\").otherwise(col(\"name\"))) \\\n",
    "       .withColumn(\"position\", when(col(\"position\").isNull(), \"Unknown\").otherwise(col(\"position\"))) \\\n",
    "       .withColumn(\"source_file\", when(col(\"source_file\").isNull(), \"Unknown\").otherwise(col(\"source_file\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a4e0193e-0c62-49a6-9379-6868ba6e4021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2eb4fc9-fd8e-41c0-96fa-8e8fbeebba6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab656b4d-2c91-4539-913b-cbe3d4632366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"products\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read table\n",
    "products_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get row count\n",
    "products_count = products_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {products_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff0f1e9-2c02-4f4f-a1f1-17b05046c694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"products\"\n",
    "BATCH_ID = \"silver_products_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by product_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\n",
    "    \"category\", \"sub_category\", \"description_pt\", \"description_de\",\n",
    "    \"description_fr\", \"description_es\", \"description_en\", \"description_zh\",\n",
    "    \"color\", \"sizes\", \"source_file\"\n",
    "]\n",
    "\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle null production_cost\n",
    "df = df.withColumn(\"production_cost\", when(col(\"production_cost\").isNull(), 0.0).otherwise(col(\"production_cost\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafd7583-b479-46ef-ba6b-50570f27f3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa59037f-de98-419b-89ee-9ed35f9540c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f7613048-aadd-40a4-90b2-1186eb464b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"stores\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read table\n",
    "stores_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get row count\n",
    "stores_count = stores_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {stores_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f9a934-96e3-435d-a83c-82d94bba923f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"stores\"\n",
    "BATCH_ID = \"silver_stores_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary key\n",
    "df = df.filter(col(\"store_id\").isNotNull())\n",
    "\n",
    "# Deduplicate by store_id (keep latest ingestion)\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"store_id\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\"country\", \"city\", \"store_name\", \"zip_code\", \"source_file\"]\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle nulls / default values for numeric columns\n",
    "df = df.withColumn(\"number_of_employees\", when(col(\"number_of_employees\").isNull(), 0).otherwise(col(\"number_of_employees\")))\n",
    "df = df.withColumn(\"latitude\", when(col(\"latitude\").isNull(), 0.0).otherwise(col(\"latitude\")))\n",
    "df = df.withColumn(\"longitude\", when(col(\"longitude\").isNull(), 0.0).otherwise(col(\"longitude\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5d91607-8070-4b82-97ce-74c38b0f9dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a9ba93-63ee-4aa0-beda-2d3bba123718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TRANSACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19bde7c-8ba4-4931-b652-1b148409e704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ecommerce_catalog\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"transactions\"\n",
    "\n",
    "# Fully qualified table name\n",
    "full_table_name = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Read table\n",
    "transactions_df = spark.read.table(full_table_name)\n",
    "\n",
    "# Get row count\n",
    "transactions_count = transactions_df.count()\n",
    "\n",
    "print(f\"Total records in {full_table_name}: {transactions_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a56d68e-8b12-4689-8110-77e7aed73562",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768373439246}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, when, current_timestamp, lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"ecommerce_catalog\"\n",
    "BRONZE_SCHEMA = \"ecommerce_bronze\"\n",
    "SILVER_SCHEMA = \"ecommerce_silver\"\n",
    "TABLE_NAME = \"transactions\"\n",
    "BATCH_ID = \"silver_transactions_batch_001\"\n",
    "\n",
    "BRONZE_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure catalog & Silver database exist\n",
    "# -----------------------------\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_SCHEMA}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read Bronze table\n",
    "# -----------------------------\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Cleaning & Transformation\n",
    "# -----------------------------\n",
    "# Normalize empty strings to NULL\n",
    "df = df_bronze.select(\n",
    "    *[when(trim(col(c)) == \"\", None).otherwise(col(c)).alias(c) for c in df_bronze.columns]\n",
    ")\n",
    "\n",
    "# Remove rows with null primary keys (invoice_id & line)\n",
    "df = df.filter(col(\"invoice_id\").isNotNull() & col(\"line\").isNotNull())\n",
    "\n",
    "# Deduplicate using composite key: invoice_id + line\n",
    "df = df.withColumn(\"tech_ingest_ts\", current_timestamp())\n",
    "window_spec = Window.partitionBy(\"invoice_id\", \"line\").orderBy(col(\"tech_ingest_ts\").desc())\n",
    "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "       .filter(col(\"rn\") == 1) \\\n",
    "       .drop(\"rn\", \"tech_ingest_ts\")\n",
    "\n",
    "# Handle nulls / default values for string columns\n",
    "string_columns = [\n",
    "    \"size\", \"color\", \"currency\", \"currency_symbol\", \"sku\",\n",
    "    \"transaction_type\", \"payment_method\"\n",
    "]\n",
    "for c in string_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), \"Unknown\").otherwise(col(c)))\n",
    "\n",
    "# Handle nulls / default values for numeric columns\n",
    "numeric_columns = [\n",
    "    \"unit_price\", \"quantity\", \"discount\", \"line_total\", \n",
    "    \"customer_id\", \"product_id\", \"store_id\", \"employee_id\", \"invoice_total\"\n",
    "]\n",
    "for c in numeric_columns:\n",
    "    df = df.withColumn(c, when(col(c).isNull(), 0).otherwise(col(c)))\n",
    "\n",
    "# Handle null date\n",
    "df = df.withColumn(\"date\", when(col(\"date\").isNull(), current_timestamp()).otherwise(col(\"date\")))\n",
    "\n",
    "# -----------------------------\n",
    "# Add audit columns\n",
    "# -----------------------------\n",
    "df = df.withColumn(\"batch_id\", lit(BATCH_ID)) \\\n",
    "       .withColumn(\"record_created_at\", current_timestamp()) \\\n",
    "       .withColumn(\"record_updated_at\", current_timestamp())\n",
    "\n",
    "# -----------------------------\n",
    "# Write to Silver table\n",
    "# -----------------------------\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify Silver table\n",
    "# -----------------------------\n",
    "display(spark.table(SILVER_TABLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e52b0444-ef2a-423e-826f-b0ae5a19f590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673f028a-b834-485b-87d6-38ce5c4a72aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE ecommerce_catalog.ecommerce_silver.transactions;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5171278232069601,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
